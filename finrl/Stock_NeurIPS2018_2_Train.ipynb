{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMjwq6pS-kFz"
   },
   "source": [
    "# Stock NeurIPS2018 Part 2. Train\n",
    "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
    "\n",
    "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
    "\n",
    "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stock NeurIPS2018 Part 2. 학습\n",
    "이 시리즈는 Practical Deep Reinforcement Learning Approach for Stock Trading 논문에서 제시한 과정을 재현한 것입니다.\n",
    "\n",
    "NeurIPS2018 시리즈의 두 번째 파트로, FinRL을 사용하여 데이터를 Gym 형식의 환경으로 변환하고, 그 환경에서 DRL 에이전트를 학습시키는 방법을 소개합니다.\n",
    "\n",
    "기타 데모들은 FinRL-Tutorials 저장소에서 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT-zXutMgqOS"
   },
   "source": [
    "# Part 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D0vEcPxSJ8hI"
   },
   "outputs": [],
   "source": [
    "# ## install finrl library\n",
    "# !pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xt1317y2ixSS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   },
   "source": [
    "# Part 2. Build A Market Environment in OpenAI Gym-style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiHhM2U-XBMZ"
   },
   "source": [
    "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeneTRdyZDvy"
   },
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3H88JXkI93v"
   },
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📈 Stock Trading with Reinforcement Learning (NeurIPS 2018 - Part 2)\n",
    "이 시리즈는 논문 Practical Deep Reinforcement Learning Approach for Stock Trading의 과정을 재현한 것입니다. 이번 파트에서는 FinRL을 활용하여 데이터를 Gym 형식의 환경으로 구성하고, 그 위에서 DRL 에이전트를 학습시키는 방법을 다룹니다.\n",
    "\n",
    "기타 예제는 FinRL-Tutorials에서 확인할 수 있습니다.\n",
    "\n",
    "# 🧠 강화학습의 핵심 요소\n",
    "\n",
    "강화학습(RL, Reinforcement Learning)의 핵심 요소는 **에이전트(Agent)**와 **환경(Environment)**입니다. RL은 다음과 같은 순환 과정으로 이해할 수 있습니다:\n",
    "\n",
    "에이전트는 환경 안에서 활동합니다.\n",
    "\n",
    "현재 상황을 **상태(state)**로 관찰합니다.\n",
    "\n",
    "**행동(action)**을 선택하여 실행합니다.\n",
    "\n",
    "그 결과로 새로운 상태로 이동하며,\n",
    "\n",
    "환경으로부터 **보상(reward)**을 받습니다.\n",
    "\n",
    "이러한 상호작용은 아래 그림처럼 반복되며, 에이전트는 보상을 최대화하는 방향으로 학습하게 됩니다.\n",
    "\n",
    "# 🏗️ Gym 스타일 환경 구성\n",
    "Python에서는 이 과정을 OpenAI Gym 스타일로 구성하여 실현합니다. 즉, 주식 데이터를 Gym 형식의 환경으로 구성하고, 상태-행동-보상을 다음과 같이 정의합니다.\n",
    "\n",
    "✅ 상태 (State, s)\n",
    "상태 공간은 에이전트가 시장을 인식하는 방식을 나타냅니다.\n",
    "\n",
    "인간 트레이더가 과거 가격 데이터와 기술적 지표를 분석하듯, 에이전트도 이를 관찰합니다.\n",
    "\n",
    "학습은 과거 데이터를 재생하며 환경과 상호작용하는 방식으로 진행됩니다.\n",
    "\n",
    "🔁 행동 (Action, a)\n",
    "행동 공간은 각 상태에서 에이전트가 취할 수 있는 행동을 정의합니다.\n",
    "\n",
    "예: 𝑎 ∈ {−1, 0, 1}\n",
    "\n",
    "−1: 매도\n",
    "\n",
    "0: 유지\n",
    "\n",
    "1: 매수\n",
    "\n",
    "다수의 주식을 거래하는 경우: 𝑎 ∈ {−𝑘, ..., −1, 0, 1, ..., 𝑘}\n",
    "예: \"AAPL 주식 10주 매수\" → 10, \"AAPL 주식 10주 매도\" → −10\n",
    "\n",
    "# 💰 보상 (Reward, r(s, a, s′))\n",
    "보상은 에이전트가 더 나은 정책을 학습하도록 유도하는 수치입니다.\n",
    "\n",
    "보상 예시: 포트폴리오 가치의 변화량\n",
    "\n",
    "𝑟(𝑠, 𝑎, 𝑠′) = 𝑣′ − 𝑣\n",
    "\n",
    "여기서 𝑣, 𝑣′는 각각 상태 𝑠와 𝑠′에서의 포트폴리오 가치\n",
    "\n",
    "# 🌍 시장 환경\n",
    "사용 데이터: **다우존스 산업평균지수(DJIA)**의 30개 구성 종목\n",
    "\n",
    "테스트 시작 시점의 데이터를 기준으로 구성된 시장 환경\n",
    "\n",
    "# 🎯 에이전트의 목표\n",
    "에이전트의 목표는 **누적 보상(cumulative reward)**을 가능한 한 많이 얻는 것입니다.\n",
    "강화학습은 이 목표를 달성하기 위해 에이전트가 자신의 행동 방식을 지속적으로 개선하는 학습 방법입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKyZejI0fmp1"
   },
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mFCP1YEhi6oi"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw95ZMicgEyi"
   },
   "source": [
    "## Construct the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WZ6-9q2gq9S"
   },
   "source": [
    "Calculate and specify the parameters we need for constructing the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"환경을 구성하기 위해 필요한 파라미터를 계산하고 구체적으로 설정하세요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WsOLoeNcJF8Q"
   },
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.005] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, #보유할 수 있는 최대 주식수\n",
    "    \"initial_amount\": 100000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7We-q73jjaFQ"
   },
   "source": [
    "## Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: DRL 에이전트 학습\n",
    "여기서 사용하는 DRL 알고리즘은 **Stable Baselines 3**에서 가져온 것입니다.\n",
    "이 라이브러리는 PyTorch 기반으로 널리 사용되는 DRL 알고리즘들을 구현한 것으로, 이전 버전인 Stable Baselines의 후속입니다.\n",
    "\n",
    "사용자들은 ElegantRL 및 **Ray RLlib**도 함께 시도해보는 것을 권장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "364PsqckttcQ"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDmqOyF9h1iz"
   },
   "source": [
    "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to results/a2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cksgh8511/anaconda3/envs/finrl38/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GVpkWGqH4-D",
    "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 114        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 23.7       |\n",
      "|    reward             | -1.4218593 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 8.91       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 118        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -191       |\n",
      "|    reward             | -16.517324 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 59.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 119       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 2.98e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -3.03e+03 |\n",
      "|    reward             | 60.59557  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.98e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 120       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 1.13e+03  |\n",
      "|    reward             | -5.800919 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 951       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 119       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 1.65e+04  |\n",
      "|    reward             | -257.2608 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.04e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 120        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 127        |\n",
      "|    reward             | -2.1036217 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 14.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 120       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -607      |\n",
      "|    reward             | 29.903273 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 407       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 120       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 1.3e+03   |\n",
      "|    reward             | 17.271952 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.22e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 120       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 20.2      |\n",
      "|    reward             | 74.438065 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 216       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 120       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 2.78e+03  |\n",
      "|    reward             | 33.310757 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.18e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 6.56e+03  |\n",
      "|    reward             | 116.85936 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.89e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 133       |\n",
      "|    reward             | 12.702839 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 22.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 53         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -261       |\n",
      "|    reward             | -3.2932913 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 127        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 5.4e+03   |\n",
      "|    reward             | 12.101263 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.11e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 61        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -3.43e+03 |\n",
      "|    reward             | -52.90107 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.23e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -5.23e+03 |\n",
      "|    reward             | 577.24316 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.14e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 256        |\n",
      "|    reward             | -3.8273697 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 57.6       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 121      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 74       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 1.04e+03 |\n",
      "|    reward             | 41.73642 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 2.15e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 78         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -184       |\n",
      "|    reward             | -61.483963 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 927        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 82        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -5.06e+03 |\n",
      "|    reward             | 43.809635 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.17e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 2100       |\n",
      "|    time_elapsed       | 86         |\n",
      "|    total_timesteps    | 10500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2099       |\n",
      "|    policy_loss        | 4.47e+03   |\n",
      "|    reward             | -20.144388 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.64e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 90        |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | 8.14e+03  |\n",
      "|    reward             | -18.58594 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 5.19e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 94        |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | 845       |\n",
      "|    reward             | 2.7828453 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 888       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 98        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | -815      |\n",
      "|    reward             | 2.8791382 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 421       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | 1.07e+03  |\n",
      "|    reward             | 61.802235 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.37e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 121      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 107      |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 767      |\n",
      "|    reward             | 5.047703 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 463      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 111       |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | 4.01e+03  |\n",
      "|    reward             | 205.32626 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.67e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | -1.28     |\n",
      "|    reward             | 11.647865 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 21.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 2900       |\n",
      "|    time_elapsed       | 119        |\n",
      "|    total_timesteps    | 14500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2899       |\n",
      "|    policy_loss        | -2.47e+03  |\n",
      "|    reward             | -23.803284 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.51e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | -1.02e+03 |\n",
      "|    reward             | 23.238213 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.24e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 127        |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | 2.03e+03   |\n",
      "|    reward             | -15.399319 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.12e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 131        |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | -1.02e+03  |\n",
      "|    reward             | -55.506824 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.57e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 121      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 135      |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | 703      |\n",
      "|    reward             | 6.276336 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.19e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 139       |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | -35       |\n",
      "|    reward             | 14.22668  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 217       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 143       |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | -3.07e+03 |\n",
      "|    reward             | 50.25026  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.84e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 148       |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | -2.58e+03 |\n",
      "|    reward             | 84.52327  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 8.04e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 152       |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | 1.59e+04  |\n",
      "|    reward             | 67.40179  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.13e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 156       |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | -1.36e+04 |\n",
      "|    reward             | 195.41714 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.39e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 160       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | -145      |\n",
      "|    reward             | 1.7484976 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 41.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 164        |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | 598        |\n",
      "|    reward             | -37.385456 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 398        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 168       |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | -1.78e+03 |\n",
      "|    reward             | 29.74944  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.38e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 172       |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4199      |\n",
      "|    policy_loss        | 4.65e+03  |\n",
      "|    reward             | 10.980396 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.86e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 177       |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | 4.5e+03   |\n",
      "|    reward             | -16.85236 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.79e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 181       |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | -1.66e+04 |\n",
      "|    reward             | 294.49826 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.96e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 185       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | -939      |\n",
      "|    reward             | 5.1236396 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 753       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 189        |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | -2.61e+03  |\n",
      "|    reward             | -15.528395 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.41e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 193       |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | -2.25e+03 |\n",
      "|    reward             | 20.541805 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.72e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 197       |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 6.32e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | 275       |\n",
      "|    reward             | 31.537064 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.9e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 201       |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | 3.89e+03  |\n",
      "|    reward             | 45.856693 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.95e+04  |\n",
      "-------------------------------------\n",
      "day: 2767, episode: 10\n",
      "begin_total_asset: 100000000.00\n",
      "end_total_asset: 254133532.69\n",
      "total_reward: 154133532.69\n",
      "total_cost: 898019.41\n",
      "total_trades: 56109\n",
      "Sharpe: 0.960\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 5000       |\n",
      "|    time_elapsed       | 205        |\n",
      "|    total_timesteps    | 25000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4999       |\n",
      "|    policy_loss        | 116        |\n",
      "|    reward             | -4.5496235 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 9.57       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 209        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 1.67e+03   |\n",
      "|    reward             | -11.557529 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.42e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 5200       |\n",
      "|    time_elapsed       | 213        |\n",
      "|    total_timesteps    | 26000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5199       |\n",
      "|    policy_loss        | 1.14e+03   |\n",
      "|    reward             | -38.062565 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.34e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 218       |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | -1.99e+03 |\n",
      "|    reward             | -55.84879 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.15e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 5400      |\n",
      "|    time_elapsed       | 222       |\n",
      "|    total_timesteps    | 27000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5399      |\n",
      "|    policy_loss        | 73.1      |\n",
      "|    reward             | 110.29792 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 61.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 226        |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | -789       |\n",
      "|    reward             | -155.66653 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.34e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 230       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | 78.6      |\n",
      "|    reward             | 3.0295086 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 15.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 234       |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | 352       |\n",
      "|    reward             | -37.64381 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 711       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 5800        |\n",
      "|    time_elapsed       | 238         |\n",
      "|    total_timesteps    | 29000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5799        |\n",
      "|    policy_loss        | -936        |\n",
      "|    reward             | -0.35591412 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.17e+03    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 121      |\n",
      "|    iterations         | 5900     |\n",
      "|    time_elapsed       | 242      |\n",
      "|    total_timesteps    | 29500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 6.54e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | 6.2e+03  |\n",
      "|    reward             | 69.49736 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 3.61e+04 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 247        |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | -2.04e+03  |\n",
      "|    reward             | -581.02856 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.04e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 6100       |\n",
      "|    time_elapsed       | 251        |\n",
      "|    total_timesteps    | 30500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6099       |\n",
      "|    policy_loss        | 316        |\n",
      "|    reward             | -3.5780022 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 60         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 6200       |\n",
      "|    time_elapsed       | 255        |\n",
      "|    total_timesteps    | 31000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6199       |\n",
      "|    policy_loss        | -627       |\n",
      "|    reward             | -13.256073 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 437        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 259       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 4.17e+03  |\n",
      "|    reward             | 15.711871 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.09e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 263       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 6.73e+03  |\n",
      "|    reward             | 33.116215 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.83e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 267       |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | 4.42e+03  |\n",
      "|    reward             | -12.41548 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.91e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 271       |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | 2.06e+04  |\n",
      "|    reward             | 1.075029  |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.83e+05  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 121         |\n",
      "|    iterations         | 6700        |\n",
      "|    time_elapsed       | 275         |\n",
      "|    total_timesteps    | 33500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6699        |\n",
      "|    policy_loss        | 477         |\n",
      "|    reward             | 0.043741364 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 160         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 279       |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | 1.85e+03  |\n",
      "|    reward             | 25.303755 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 2.51e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 283       |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6899      |\n",
      "|    policy_loss        | -8.86e+03 |\n",
      "|    reward             | 11.877355 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.97e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 287       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | -1.19e+03 |\n",
      "|    reward             | 42.253845 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.83e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 7100       |\n",
      "|    time_elapsed       | 291        |\n",
      "|    total_timesteps    | 35500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7099       |\n",
      "|    policy_loss        | 1.28e+04   |\n",
      "|    reward             | -458.15085 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 1.12e+05   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 296        |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | -149       |\n",
      "|    reward             | 0.72941244 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 14.5       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 121      |\n",
      "|    iterations         | 7300     |\n",
      "|    time_elapsed       | 300      |\n",
      "|    total_timesteps    | 36500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7299     |\n",
      "|    policy_loss        | 617      |\n",
      "|    reward             | 7.180016 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 290      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 121      |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 304      |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | 902      |\n",
      "|    reward             | 9.369647 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 703      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 308       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | -1.32e+04 |\n",
      "|    reward             | -82.0405  |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 9.53e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 7600       |\n",
      "|    time_elapsed       | 312        |\n",
      "|    total_timesteps    | 38000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7599       |\n",
      "|    policy_loss        | -3.35e+03  |\n",
      "|    reward             | -39.493214 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 6.68e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 316       |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | -5.69e+04 |\n",
      "|    reward             | -97.66777 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.95e+06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 320       |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7799      |\n",
      "|    policy_loss        | -231      |\n",
      "|    reward             | 0.6758356 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 95.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 324       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | -2.18e+03 |\n",
      "|    reward             | 87.7417   |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.1e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 328       |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | 1.38e+03  |\n",
      "|    reward             | 2.4347625 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.74e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 332       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | 6.53e+03  |\n",
      "|    reward             | -288.9775 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.27e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 336        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | 1.49e+04   |\n",
      "|    reward             | -39.111805 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.54e+05   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 341        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | 1.29e+04   |\n",
      "|    reward             | -211.27309 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 9.23e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 8400      |\n",
      "|    time_elapsed       | 345       |\n",
      "|    total_timesteps    | 42000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8399      |\n",
      "|    policy_loss        | 769       |\n",
      "|    reward             | 1.4221375 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 552       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 349       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | -2.42e+03 |\n",
      "|    reward             | 58.856907 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 4.83e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 353       |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | 2.41e+03  |\n",
      "|    reward             | 56.484764 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 6.41e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 357       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | -7.32e+03 |\n",
      "|    reward             | 23.976505 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.57e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 361       |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | 1.85e+04  |\n",
      "|    reward             | 662.9041  |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.46e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 121      |\n",
      "|    iterations         | 8900     |\n",
      "|    time_elapsed       | 365      |\n",
      "|    total_timesteps    | 44500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8899     |\n",
      "|    policy_loss        | -448     |\n",
      "|    reward             | 4.871693 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 141      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 9000      |\n",
      "|    time_elapsed       | 369       |\n",
      "|    total_timesteps    | 45000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8999      |\n",
      "|    policy_loss        | 3.14e+03  |\n",
      "|    reward             | 2.8601277 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 6.6e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 373       |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | 1.45e+03  |\n",
      "|    reward             | 30.581371 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.43e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 377       |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | -935      |\n",
      "|    reward             | -82.54608 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 5.17e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 381       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | -1.19e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | 386       |\n",
      "|    reward             | 118.25782 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 5.17e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 121      |\n",
      "|    iterations         | 9400     |\n",
      "|    time_elapsed       | 386      |\n",
      "|    total_timesteps    | 47000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.1    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | 4.36e+03 |\n",
      "|    reward             | 232.6682 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.02e+05 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 390        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | -605       |\n",
      "|    reward             | -3.2372751 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 253        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 9600      |\n",
      "|    time_elapsed       | 394       |\n",
      "|    total_timesteps    | 48000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9599      |\n",
      "|    policy_loss        | -329      |\n",
      "|    reward             | 39.826305 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 75.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 398        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 662        |\n",
      "|    reward             | -20.647646 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.61e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 402       |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 5.36e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 1.68e+03  |\n",
      "|    reward             | 105.51805 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 1.92e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 121       |\n",
      "|    iterations         | 9900      |\n",
      "|    time_elapsed       | 406       |\n",
      "|    total_timesteps    | 49500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9899      |\n",
      "|    policy_loss        | -4.06e+03 |\n",
      "|    reward             | 124.65222 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 1.93e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 121        |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 410        |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -40.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | 38.9       |\n",
      "|    reward             | -3.0389903 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 21.5       |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zjCWfgsg3sVa"
   },
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ 2. 학습 결과 비교\n",
    "📘 A2C 결과 요약 (총 50,000 timestep 기준)\n",
    "reward: -0.2 (학습 거의 실패)\n",
    "\n",
    "explained_variance: 0 → value function 작동 실패\n",
    "\n",
    "entropy_loss: 매우 낮음 → 탐색 부족\n",
    "\n",
    "policy_loss: 계속 큼 → 정책 불안정\n",
    "\n",
    "요약: 정책, critic 모두 학습 실패\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "M2YadjfnLwgt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cuda device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tCDa78rqfO_a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 79       |\n",
      "|    time_elapsed    | 138      |\n",
      "|    total_timesteps | 11072    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.59e+04 |\n",
      "|    critic_loss     | 1.38e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10971    |\n",
      "|    reward          | 80.82665 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 79       |\n",
      "|    time_elapsed    | 277      |\n",
      "|    total_timesteps | 22144    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.15e+04 |\n",
      "|    critic_loss     | 5.62e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 22043    |\n",
      "|    reward          | 80.82665 |\n",
      "---------------------------------\n",
      "day: 2767, episode: 30\n",
      "begin_total_asset: 100000000.00\n",
      "end_total_asset: 331801325.20\n",
      "total_reward: 231801325.20\n",
      "total_cost: 497512.43\n",
      "total_trades: 38738\n",
      "Sharpe: 1.132\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 79       |\n",
      "|    time_elapsed    | 416      |\n",
      "|    total_timesteps | 33216    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.41e+04 |\n",
      "|    critic_loss     | 1.26e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 33115    |\n",
      "|    reward          | 80.82665 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 79       |\n",
      "|    time_elapsed    | 555      |\n",
      "|    total_timesteps | 44288    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.69e+03 |\n",
      "|    critic_loss     | 1.52e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 44187    |\n",
      "|    reward          | 80.82665 |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ne6M2R-WvrUQ"
   },
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실패\n",
    "\n",
    "📙 DD-PPO 결과 요약 (46,288 timestep 기준)\n",
    "reward: 7.69 ✅ (지속적인 양의 보상, 성공적인 학습)\n",
    "\n",
    "critic_loss: 6.12 (value function도 안정화됨)\n",
    "\n",
    "actor_loss: -18.7 (수렴 중)\n",
    "\n",
    "n_updates ≈ timesteps: 학습이 매우 빠르고 효율적\n",
    "\n",
    "요약: 정책 수렴 + critic 안정화 → 실제 환경에서 유용한 정책 생성 성공\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실험에서 A2C는 수렴 실패, DD-PPO는 보상 수렴과 critic 안정화 모두 성공했으므로, 실용적 강화학습에선 DD-PPO 압승."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "y5D5PFUhMzSV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to results/ppo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cksgh8511/anaconda3/envs/finrl38/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Gt8eIQKYM4G3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 138        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 14         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.50900334 |\n",
      "-----------------------------------\n",
      "day: 2767, episode: 40\n",
      "begin_total_asset: 100000000.00\n",
      "end_total_asset: 106161970.61\n",
      "total_reward: 6161970.61\n",
      "total_cost: 1592356.61\n",
      "total_trades: 78627\n",
      "Sharpe: 0.626\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008867039 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.2        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00819    |\n",
      "|    reward               | 2.8267758   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 53          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009828206 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 106         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00797    |\n",
      "|    reward               | -0.1957165  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 244         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010709258 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 78.2        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00747    |\n",
      "|    reward               | -3.7771337  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 211         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009485256 |\n",
      "|    clip_fraction        | 0.0992      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 125         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00552    |\n",
      "|    reward               | 14.775315   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 231         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009212322 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 66          |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.009      |\n",
      "|    reward               | -2.9867523  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 157         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007331662 |\n",
      "|    clip_fraction        | 0.0333      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 258         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0057     |\n",
      "|    reward               | 1.5486543   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 625         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 127         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006808239 |\n",
      "|    clip_fraction        | 0.0325      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 293         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00472    |\n",
      "|    reward               | 31.398438   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 688         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 143         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013083464 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 230         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00683    |\n",
      "|    reward               | -4.8667345  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 600         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 159         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012369528 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 96.1        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00748    |\n",
      "|    reward               | -3.2332008  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 307         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 176          |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.009431662  |\n",
      "|    clip_fraction        | 0.059        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.4        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 309          |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00494     |\n",
      "|    reward               | -0.033732783 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 714          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 192         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009998217 |\n",
      "|    clip_fraction        | 0.0936      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -2.38e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 603         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00563    |\n",
      "|    reward               | -13.51956   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 1.2e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 208         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009857964 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 211         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00786    |\n",
      "|    reward               | -10.451119  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 451         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 224         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012435837 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 426         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00804    |\n",
      "|    reward               | 2.5279226   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 617         |\n",
      "-----------------------------------------\n",
      "day: 2767, episode: 50\n",
      "begin_total_asset: 100000000.00\n",
      "end_total_asset: 115961207.36\n",
      "total_reward: 15961207.36\n",
      "total_cost: 1590305.80\n",
      "total_trades: 77769\n",
      "Sharpe: 0.791\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 240         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010002326 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 680         |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00463    |\n",
      "|    reward               | 1.5266997   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 950         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 256         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006905454 |\n",
      "|    clip_fraction        | 0.0462      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 715         |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00644    |\n",
      "|    reward               | -65.5437    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 1.21e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 273         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013132772 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 210         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00775    |\n",
      "|    reward               | 3.2988586   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 440         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 289         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011034561 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | 1.79e-07    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 435         |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00715    |\n",
      "|    reward               | 3.8552442   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 652         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 305         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008076849 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 475         |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00815    |\n",
      "|    reward               | 1.0650223   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 1.47e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 321        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01030184 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.9      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 314        |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    reward               | 4.4964705  |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 664        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 337         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008962741 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 95.3        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00851    |\n",
      "|    reward               | -4.511916   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 163         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 353         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009564738 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42         |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 257         |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00983    |\n",
      "|    reward               | 0.24447845  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 602         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 369         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010530366 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 283         |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00936    |\n",
      "|    reward               | 0.3238348   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 729         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 385         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009219142 |\n",
      "|    clip_fraction        | 0.0925      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 405         |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00751    |\n",
      "|    reward               | 2.0757396   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 573         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 401         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008900287 |\n",
      "|    clip_fraction        | 0.0622      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 36.7        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00848    |\n",
      "|    reward               | 0.8810658   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 84.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 417         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010881221 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 148         |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00888    |\n",
      "|    reward               | -7.7120004  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 353         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 433         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010718493 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 192         |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    reward               | -29.25153   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 509         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 449         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008702686 |\n",
      "|    clip_fraction        | 0.0899      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 229         |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00845    |\n",
      "|    reward               | 10.092265   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 501         |\n",
      "-----------------------------------------\n",
      "day: 2767, episode: 60\n",
      "begin_total_asset: 100000000.00\n",
      "end_total_asset: 112212738.51\n",
      "total_reward: 12212738.51\n",
      "total_cost: 1565853.34\n",
      "total_trades: 76427\n",
      "Sharpe: 0.721\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 465         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009053581 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 80.5        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    reward               | 0.6017427   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 191         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 481         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009715481 |\n",
      "|    clip_fraction        | 0.0828      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 420         |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00792    |\n",
      "|    reward               | 0.29648083  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 761         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 497         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007923465 |\n",
      "|    clip_fraction        | 0.088       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 251         |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00848    |\n",
      "|    reward               | 14.409879   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 491         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 513         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013056819 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 338         |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    reward               | -5.710138   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 756         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 529         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013317326 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 186         |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    reward               | 4.827186    |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 301         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 545         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011240354 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 264         |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    reward               | -0.83014464 |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 738         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 561         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009282102 |\n",
      "|    clip_fraction        | 0.0845      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 612         |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00496    |\n",
      "|    reward               | 41.28091    |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 1.14e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 578         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012954768 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 262         |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00881    |\n",
      "|    reward               | -7.4129076  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 636         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 594         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013076512 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 293         |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0086     |\n",
      "|    reward               | -4.2705927  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 515         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 610         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017100349 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 665         |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00941    |\n",
      "|    reward               | 0.58300596  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 1.28e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 626         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007652181 |\n",
      "|    clip_fraction        | 0.0578      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 926         |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00412    |\n",
      "|    reward               | -28.078094  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 2.25e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 642          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071599297 |\n",
      "|    clip_fraction        | 0.0935       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -43.1        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 445          |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00675     |\n",
      "|    reward               | 4.7691646    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 1.04e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 658         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016725946 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 639         |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00979    |\n",
      "|    reward               | 1.0590006   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 1.19e+03    |\n",
      "-----------------------------------------\n",
      "day: 2767, episode: 70\n",
      "begin_total_asset: 100000000.00\n",
      "end_total_asset: 117266759.43\n",
      "total_reward: 17266759.43\n",
      "total_cost: 1582853.81\n",
      "total_trades: 76080\n",
      "Sharpe: 0.689\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 674          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075415205 |\n",
      "|    clip_fraction        | 0.102        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -43.2        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 829          |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00789     |\n",
      "|    reward               | 3.3307164    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 1.87e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 690         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007417723 |\n",
      "|    clip_fraction        | 0.0812      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.34e+03    |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00697    |\n",
      "|    reward               | 13.133934   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 2.04e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 706         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008785952 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 470         |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00602    |\n",
      "|    reward               | 10.961795   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 594         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 722         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011044655 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 594         |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00718    |\n",
      "|    reward               | -1.0497388  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 1.2e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 738         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008509592 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 902         |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00733    |\n",
      "|    reward               | -0.28901383 |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 2.29e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 754         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009073555 |\n",
      "|    clip_fraction        | 0.0657      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.18e+03    |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00476    |\n",
      "|    reward               | -1.2592955  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 2.06e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 770        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01541558 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.5      |\n",
      "|    explained_variance   | 5.96e-08   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 241        |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    reward               | 1.7833593  |\n",
      "|    std                  | 1.08       |\n",
      "|    value_loss           | 479        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 786         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010723116 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 505         |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    reward               | 7.233857    |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 1.32e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 802         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007718005 |\n",
      "|    clip_fraction        | 0.0838      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 481         |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.00813    |\n",
      "|    reward               | -10.832957  |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 1.52e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 818         |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010438487 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 753         |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    reward               | -4.1343746  |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 1.37e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 834          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058699497 |\n",
      "|    clip_fraction        | 0.0971       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -43.7        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 149          |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.00757     |\n",
      "|    reward               | 0.36501232   |\n",
      "|    std                  | 1.1          |\n",
      "|    value_loss           | 367          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 850         |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008681564 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 426         |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.00697    |\n",
      "|    reward               | 1.8693378   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 1.25e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 866         |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009436487 |\n",
      "|    clip_fraction        | 0.0896      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.16e+03    |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00798    |\n",
      "|    reward               | 17.316591   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 2.34e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 882         |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009216484 |\n",
      "|    clip_fraction        | 0.0959      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 866         |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    reward               | -1.5119061  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 1.8e+03     |\n",
      "-----------------------------------------\n",
      "day: 2767, episode: 80\n",
      "begin_total_asset: 100000000.00\n",
      "end_total_asset: 120387952.88\n",
      "total_reward: 20387952.88\n",
      "total_cost: 1587584.13\n",
      "total_trades: 76353\n",
      "Sharpe: 0.735\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 898         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009934863 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 216         |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00543    |\n",
      "|    reward               | -1.6909742  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 595         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 914         |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009831995 |\n",
      "|    clip_fraction        | 0.0881      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 793         |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.00743    |\n",
      "|    reward               | -0.65469164 |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 2.17e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 58         |\n",
      "|    time_elapsed         | 930        |\n",
      "|    total_timesteps      | 118784     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01011219 |\n",
      "|    clip_fraction        | 0.0991     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.9      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 857        |\n",
      "|    n_updates            | 570        |\n",
      "|    policy_gradient_loss | -0.00526   |\n",
      "|    reward               | 54.688866  |\n",
      "|    std                  | 1.1        |\n",
      "|    value_loss           | 2.63e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 946         |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010477154 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 823         |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00371    |\n",
      "|    reward               | -2.7045782  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 2.11e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 963         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007565299 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 647         |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.00692    |\n",
      "|    reward               | -5.7980714  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 1.37e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 979         |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011533457 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.68e+03    |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.00624    |\n",
      "|    reward               | -0.43033996 |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 2.99e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 995          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052037896 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44          |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.21e+03     |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.00185     |\n",
      "|    reward               | -18.149164   |\n",
      "|    std                  | 1.1          |\n",
      "|    value_loss           | 3.73e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 1011        |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012072459 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 989         |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.00887    |\n",
      "|    reward               | 13.305544   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 1.66e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 1027         |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067234566 |\n",
      "|    clip_fraction        | 0.126        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44          |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.03e+03     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.0038      |\n",
      "|    reward               | 1.8394111    |\n",
      "|    std                  | 1.11         |\n",
      "|    value_loss           | 1.93e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 1043        |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013491131 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.56e+03    |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.00532    |\n",
      "|    reward               | 0.019398157 |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 4.09e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 1059        |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010184659 |\n",
      "|    clip_fraction        | 0.0903      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.5e+03     |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00633    |\n",
      "|    reward               | -81.162674  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 4.4e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 1075        |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010395748 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 683         |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00851    |\n",
      "|    reward               | -8.124662   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 2.02e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 1091        |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010421867 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.76e+03    |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00571    |\n",
      "|    reward               | 1.765032    |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 3.09e+03    |\n",
      "-----------------------------------------\n",
      "day: 2767, episode: 90\n",
      "begin_total_asset: 100000000.00\n",
      "end_total_asset: 124109511.17\n",
      "total_reward: 24109511.17\n",
      "total_cost: 1597663.57\n",
      "total_trades: 76132\n",
      "Sharpe: 0.777\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 69         |\n",
      "|    time_elapsed         | 1107       |\n",
      "|    total_timesteps      | 141312     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01195698 |\n",
      "|    clip_fraction        | 0.101      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.2      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 2.24e+03   |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.00592   |\n",
      "|    reward               | 0.50295943 |\n",
      "|    std                  | 1.12       |\n",
      "|    value_loss           | 3.47e+03   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 1123         |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061339783 |\n",
      "|    clip_fraction        | 0.0921       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.3        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.27e+03     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00687     |\n",
      "|    reward               | 13.184707    |\n",
      "|    std                  | 1.12         |\n",
      "|    value_loss           | 2.99e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 1139        |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013137097 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 273         |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.00793    |\n",
      "|    reward               | 6.1876535   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 581         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 1155        |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008872755 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 635         |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.00767    |\n",
      "|    reward               | -0.18624233 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 1.83e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 1171        |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00901722  |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 950         |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.00653    |\n",
      "|    reward               | -0.09362775 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 1.85e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 1187        |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013135906 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 769         |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00403    |\n",
      "|    reward               | -0.31931236 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 1.98e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 1203        |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012672743 |\n",
      "|    clip_fraction        | 0.0942      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 230         |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.00917    |\n",
      "|    reward               | 0.20914824  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 513         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 1219         |\n",
      "|    total_timesteps      | 155648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086418195 |\n",
      "|    clip_fraction        | 0.134        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.5        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.03e+03     |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.00702     |\n",
      "|    reward               | 2.4047065    |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 1.61e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 1235        |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010224503 |\n",
      "|    clip_fraction        | 0.0912      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.6       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.41e+03    |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.00647    |\n",
      "|    reward               | -8.928149   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 3.36e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 1251        |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010678472 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.6       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.23e+03    |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00746    |\n",
      "|    reward               | 21.955177   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 2.3e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 1267        |\n",
      "|    total_timesteps      | 161792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008630101 |\n",
      "|    clip_fraction        | 0.0919      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 274         |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.00771    |\n",
      "|    reward               | -1.9526541  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 686         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 1283        |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014855867 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.83e+03    |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    reward               | 2.166324    |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 3.51e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 1299        |\n",
      "|    total_timesteps      | 165888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008584237 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.29e+03    |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00268    |\n",
      "|    reward               | 35.43001    |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 3.16e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 1315        |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013734918 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.16e+03    |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00836    |\n",
      "|    reward               | -7.665449   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 2.85e+03    |\n",
      "-----------------------------------------\n",
      "day: 2767, episode: 100\n",
      "begin_total_asset: 100000000.00\n",
      "end_total_asset: 126364168.21\n",
      "total_reward: 26364168.21\n",
      "total_cost: 1650413.53\n",
      "total_trades: 77388\n",
      "Sharpe: 0.886\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 1331        |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012004882 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 513         |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.00835    |\n",
      "|    reward               | -0.8549708  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 1.2e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 1347        |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008831675 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.18e+03    |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00408    |\n",
      "|    reward               | 2.4948525   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 2.3e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 1363        |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011891058 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.18e+03    |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.00486    |\n",
      "|    reward               | 73.63256    |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 2.52e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 1379        |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013185658 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 750         |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00709    |\n",
      "|    reward               | -6.5061545  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 1.22e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 1395        |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020804446 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.1       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 595         |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00909    |\n",
      "|    reward               | 1.9307853   |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 1.51e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 1411        |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011143951 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.23e+03    |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00656    |\n",
      "|    reward               | -0.29674438 |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 2.45e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 1428         |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0120709855 |\n",
      "|    clip_fraction        | 0.136        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -45.2        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.11e+03     |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00848     |\n",
      "|    reward               | -11.091304   |\n",
      "|    std                  | 1.15         |\n",
      "|    value_loss           | 2.6e+03      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1444        |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010519236 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 377         |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.00752    |\n",
      "|    reward               | -36.984905  |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 1.3e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1460        |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011423742 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.4       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.09e+03    |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    reward               | 0.057046358 |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 1.74e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1476        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011177415 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.35e+03    |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00655    |\n",
      "|    reward               | 0.20650117  |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 2.72e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1492        |\n",
      "|    total_timesteps      | 190464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009436147 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.38e+03    |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00822    |\n",
      "|    reward               | 3.0037355   |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 2.09e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1508        |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011500519 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 297         |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | 14.193431   |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 665         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1524        |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010858589 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.6       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 748         |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.00638    |\n",
      "|    reward               | 2.4816315   |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 1.82e+03    |\n",
      "-----------------------------------------\n",
      "day: 2767, episode: 110\n",
      "begin_total_asset: 100000000.00\n",
      "end_total_asset: 127290865.53\n",
      "total_reward: 27290865.53\n",
      "total_cost: 1607542.19\n",
      "total_trades: 75614\n",
      "Sharpe: 0.857\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1540        |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009610629 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.6       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.73e+03    |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.00655    |\n",
      "|    reward               | 1.3174069   |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 2.8e+03     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 97         |\n",
      "|    time_elapsed         | 1556       |\n",
      "|    total_timesteps      | 198656     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00947087 |\n",
      "|    clip_fraction        | 0.0818     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.6      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.17e+03   |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.0031    |\n",
      "|    reward               | -6.751129  |\n",
      "|    std                  | 1.17       |\n",
      "|    value_loss           | 3.16e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1572        |\n",
      "|    total_timesteps      | 200704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008232702 |\n",
      "|    clip_fraction        | 0.0917      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.7       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 443         |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.00382    |\n",
      "|    reward               | -6.1536646  |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 901         |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "C6AidlWyvwzm"
   },
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JSAHhV4Xc-bh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
      "Using cuda device\n",
      "Logging to results/td3\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OSRxNYAxdKpU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 83        |\n",
      "|    time_elapsed    | 132       |\n",
      "|    total_timesteps | 11072     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.63e+04 |\n",
      "|    critic_loss     | 1.15e+05  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 10971     |\n",
      "|    reward          | 54.411034 |\n",
      "----------------------------------\n",
      "day: 2767, episode: 120\n",
      "begin_total_asset: 100000000.00\n",
      "end_total_asset: 289017857.50\n",
      "total_reward: 189017857.50\n",
      "total_cost: 672833.41\n",
      "total_trades: 35971\n",
      "Sharpe: 1.050\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 83        |\n",
      "|    time_elapsed    | 264       |\n",
      "|    total_timesteps | 22144     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.54e+04 |\n",
      "|    critic_loss     | 2.31e+04  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 22043     |\n",
      "|    reward          | 54.411034 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 83        |\n",
      "|    time_elapsed    | 397       |\n",
      "|    total_timesteps | 33216     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.33e+04 |\n",
      "|    critic_loss     | 1.33e+04  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 33115     |\n",
      "|    reward          | 54.411034 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 83        |\n",
      "|    time_elapsed    | 530       |\n",
      "|    total_timesteps | 44288     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.66e+04 |\n",
      "|    critic_loss     | 1.64e+04  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 44187     |\n",
      "|    reward          | 54.411034 |\n",
      "----------------------------------\n",
      "day: 2767, episode: 130\n",
      "begin_total_asset: 100000000.00\n",
      "end_total_asset: 289017857.50\n",
      "total_reward: 189017857.50\n",
      "total_cost: 672833.41\n",
      "total_trades: 35971\n",
      "Sharpe: 1.050\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OkJV6V_mv2hw"
   },
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xwOhVjqRkCdM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to results/sac\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "K8RSdKCckJyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 59       |\n",
      "|    time_elapsed    | 185      |\n",
      "|    total_timesteps | 11072    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.3e+03 |\n",
      "|    critic_loss     | 3.96e+05 |\n",
      "|    ent_coef        | 0.286    |\n",
      "|    ent_coef_loss   | 457      |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 10971    |\n",
      "|    reward          | 87.83061 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 57       |\n",
      "|    time_elapsed    | 386      |\n",
      "|    total_timesteps | 22144    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.26e+03 |\n",
      "|    critic_loss     | 1.47e+04 |\n",
      "|    ent_coef        | 0.866    |\n",
      "|    ent_coef_loss   | 53.2     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 22043    |\n",
      "|    reward          | 87.83061 |\n",
      "---------------------------------\n",
      "day: 2767, episode: 10\n",
      "begin_total_asset: 100000000.00\n",
      "end_total_asset: 253558546.08\n",
      "total_reward: 153558546.08\n",
      "total_cost: 497512.42\n",
      "total_trades: 37052\n",
      "Sharpe: 0.917\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 56       |\n",
      "|    time_elapsed    | 585      |\n",
      "|    total_timesteps | 33216    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.45e+04 |\n",
      "|    critic_loss     | 4.28e+04 |\n",
      "|    ent_coef        | 2.62     |\n",
      "|    ent_coef_loss   | -358     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 33115    |\n",
      "|    reward          | 87.83061 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 56       |\n",
      "|    time_elapsed    | 787      |\n",
      "|    total_timesteps | 44288    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.08e+05 |\n",
      "|    critic_loss     | 4.78e+05 |\n",
      "|    ent_coef        | 7.92     |\n",
      "|    ent_coef_loss   | -756     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 44187    |\n",
      "|    reward          | 87.83061 |\n",
      "---------------------------------\n",
      "day: 2767, episode: 20\n",
      "begin_total_asset: 100000000.00\n",
      "end_total_asset: 253558546.08\n",
      "total_reward: 153558546.08\n",
      "total_cost: 497512.42\n",
      "total_trades: 37052\n",
      "Sharpe: 0.917\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 55        |\n",
      "|    time_elapsed    | 988       |\n",
      "|    total_timesteps | 55360     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.35e+05  |\n",
      "|    critic_loss     | 4.87e+06  |\n",
      "|    ent_coef        | 24        |\n",
      "|    ent_coef_loss   | -1.17e+03 |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 55259     |\n",
      "|    reward          | 87.83061  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 55        |\n",
      "|    time_elapsed    | 1186      |\n",
      "|    total_timesteps | 66432     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 9.88e+05  |\n",
      "|    critic_loss     | 3.89e+06  |\n",
      "|    ent_coef        | 72.5      |\n",
      "|    ent_coef_loss   | -1.57e+03 |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 66331     |\n",
      "|    reward          | 87.83061  |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_SpZoQgPv7GO"
   },
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgGm3dQZfRks"
   },
   "source": [
    "## Save the trained agent\n",
    "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
    "\n",
    "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
    "\n",
    "For users running on your local environment, the zip files should be at \"./trained_models\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
